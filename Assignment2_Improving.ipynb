{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14de297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gx/pwy0byr15pq8m62vhfygrw8w0000gn/T/ipykernel_49491/2870729131.py:20: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# for importing data\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991a73e",
   "metadata": {},
   "source": [
    "## Basic CNN with Batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231b1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.CenterCrop(100),\n",
    "    transforms.ToTensor(), # normalizes to range [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # further normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f1325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images available: 67692\n",
      "Testing images available:  22688\n",
      "Class types available:  131\n"
     ]
    }
   ],
   "source": [
    "root = '../Data/Fruit360/fruits-360_dataset/fruits-360/'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Training'), transform=transform)\n",
    "test_data = datasets.ImageFolder(os.path.join(root, 'Test'), transform=transform)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "\n",
    "print(f'Training images available: {len(train_data)}')\n",
    "print(f'Testing images available:  {len(test_data)}')\n",
    "print(f'Class types available:  {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cea7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        # print toggle\n",
    "        self.print = False\n",
    "\n",
    "\n",
    "        ### -------------- feature map layers -------------- ###\n",
    "        # first convolution layer\n",
    "        self.conv1  = nn.Conv2d(3,32,3,stride=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(32) # input the number of channels in this layer\n",
    "      \n",
    "\n",
    "        # second convolution layer\n",
    "        self.conv2  = nn.Conv2d(32,32,3,stride=1)\n",
    "        self.bnorm2 = nn.BatchNorm2d(32) # input the number of channels in this layer\n",
    "      \n",
    "        # third convolution layer\n",
    "        self.conv3  = nn.Conv2d(32,64,3,stride=1)\n",
    "        self.bnorm3 = nn.BatchNorm2d(64) # input the number of channels in this layer\n",
    "\n",
    "\n",
    "        ### -------------- linear decision layers -------------- ###\n",
    "        self.fc1 = nn.Linear(10*10*64,1024)\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512,131)\n",
    "\n",
    "        # toggle for printing out tensor sizes during forward prop\n",
    "        self.print = False\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.print: print(f'Input: {list(x.shape)}')\n",
    "      \n",
    "        # first block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv1(x),2)\n",
    "        x = F.relu(self.bnorm1(x))\n",
    "            \n",
    "        if self.print: print(f'First CPR block: {list(x.shape)}')\n",
    "\n",
    "        # second block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv2(x),2)\n",
    "        x = F.relu(self.bnorm2(x))\n",
    "        if self.print: print(f'Second CPR block: {list(x.shape)}')\n",
    "\n",
    "        # third block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv3(x),2)\n",
    "        x = F.relu(self.bnorm3(x))\n",
    "        if self.print: print(f'Third CPR block: {list(x.shape)}')\n",
    "\n",
    "        # reshape for linear layer\n",
    "        x = x.view(-1,10*10*64)\n",
    "        if self.print: print(f'Vectorized: {list(x.shape)}')\n",
    "      \n",
    "        # linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.print: print(f'Final output: {list(x.shape)}')\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# create the model instance\n",
    "cnn = basicCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dacc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(),lr=0.01,momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e23b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  300 loss: 0.16509847 train accuracy:  54.083%\n",
      "epoch:  0  batch:  600 loss: 0.17801775 train accuracy:  72.516%\n",
      "epoch:  0  batch:  900 loss: 0.05421299 train accuracy:  80.368%\n",
      "epoch:  0  batch: 1200 loss: 0.05876272 train accuracy:  84.622%\n",
      "epoch:  0  batch: 1500 loss: 0.02339854 train accuracy:  87.306%\n",
      "epoch:  0  batch: 1800 loss: 0.00554814 train accuracy:  89.234%\n",
      "epoch:  0  batch: 2100 loss: 0.00419619 train accuracy:  90.693%\n",
      "Finished epoch 1/10. Test accuracy = 94.43%\n",
      "epoch:  1  batch:  300 loss: 0.00639857 train accuracy:  99.500%\n",
      "epoch:  1  batch:  600 loss: 0.00101474 train accuracy:  99.609%\n",
      "epoch:  1  batch:  900 loss: 0.00057422 train accuracy:  99.653%\n",
      "epoch:  1  batch: 1200 loss: 0.00212555 train accuracy:  99.703%\n",
      "epoch:  1  batch: 1500 loss: 0.05344861 train accuracy:  99.717%\n",
      "epoch:  1  batch: 1800 loss: 0.00234797 train accuracy:  99.729%\n",
      "epoch:  1  batch: 2100 loss: 0.00011228 train accuracy:  99.756%\n",
      "Finished epoch 2/10. Test accuracy = 98.45%\n",
      "epoch:  2  batch:  300 loss: 0.00010630 train accuracy:  99.948%\n",
      "epoch:  2  batch:  600 loss: 0.00021842 train accuracy:  99.943%\n",
      "epoch:  2  batch:  900 loss: 0.00127037 train accuracy:  99.885%\n",
      "epoch:  2  batch: 1200 loss: 0.01454054 train accuracy:  99.878%\n",
      "epoch:  2  batch: 1500 loss: 0.00250066 train accuracy:  99.875%\n",
      "epoch:  2  batch: 1800 loss: 0.00157476 train accuracy:  99.849%\n",
      "epoch:  2  batch: 2100 loss: 0.00167405 train accuracy:  99.841%\n",
      "Finished epoch 3/10. Test accuracy = 98.43%\n",
      "epoch:  3  batch:  300 loss: 0.00954458 train accuracy:  99.323%\n",
      "epoch:  3  batch:  600 loss: 0.02074448 train accuracy:  99.469%\n",
      "epoch:  3  batch:  900 loss: 0.00027415 train accuracy:  99.615%\n",
      "epoch:  3  batch: 1200 loss: 0.00044589 train accuracy:  99.643%\n",
      "epoch:  3  batch: 1500 loss: 0.00028190 train accuracy:  99.708%\n",
      "epoch:  3  batch: 1800 loss: 0.00016805 train accuracy:  99.755%\n",
      "epoch:  3  batch: 2100 loss: 0.00010693 train accuracy:  99.778%\n",
      "Finished epoch 4/10. Test accuracy = 98.16%\n",
      "epoch:  4  batch:  300 loss: 0.00007110 train accuracy:  99.875%\n",
      "epoch:  4  batch:  600 loss: 0.02276180 train accuracy:  99.828%\n",
      "epoch:  4  batch:  900 loss: 0.00012145 train accuracy:  99.858%\n",
      "epoch:  4  batch: 1200 loss: 0.02612630 train accuracy:  99.854%\n",
      "epoch:  4  batch: 1500 loss: 0.00101318 train accuracy:  99.873%\n",
      "epoch:  4  batch: 1800 loss: 0.00064487 train accuracy:  99.887%\n",
      "epoch:  4  batch: 2100 loss: 0.00251742 train accuracy:  99.894%\n",
      "Finished epoch 5/10. Test accuracy = 98.70%\n",
      "epoch:  5  batch:  300 loss: 0.00004587 train accuracy: 100.000%\n",
      "epoch:  5  batch:  600 loss: 0.00506208 train accuracy: 100.000%\n",
      "epoch:  5  batch:  900 loss: 0.00003134 train accuracy: 100.000%\n",
      "epoch:  5  batch: 1200 loss: 0.00043086 train accuracy:  99.997%\n",
      "epoch:  5  batch: 1500 loss: 0.00001564 train accuracy:  99.996%\n",
      "epoch:  5  batch: 1800 loss: 0.00012295 train accuracy:  99.997%\n",
      "epoch:  5  batch: 2100 loss: 0.00133560 train accuracy:  99.988%\n",
      "Finished epoch 6/10. Test accuracy = 98.36%\n",
      "epoch:  6  batch:  300 loss: 0.00023056 train accuracy: 100.000%\n",
      "epoch:  6  batch:  600 loss: 0.00014843 train accuracy:  99.995%\n",
      "epoch:  6  batch:  900 loss: 0.01136268 train accuracy:  99.993%\n",
      "epoch:  6  batch: 1200 loss: 0.00004267 train accuracy:  99.990%\n",
      "epoch:  6  batch: 1500 loss: 0.00142645 train accuracy:  99.992%\n",
      "epoch:  6  batch: 1800 loss: 0.00019708 train accuracy:  99.993%\n",
      "epoch:  6  batch: 2100 loss: 0.00001165 train accuracy:  99.994%\n",
      "Finished epoch 7/10. Test accuracy = 98.86%\n",
      "epoch:  7  batch:  300 loss: 0.00001579 train accuracy:  99.938%\n",
      "epoch:  7  batch:  600 loss: 0.00004446 train accuracy:  99.932%\n",
      "epoch:  7  batch:  900 loss: 0.00005198 train accuracy:  99.955%\n",
      "epoch:  7  batch: 1200 loss: 0.00058720 train accuracy:  99.961%\n",
      "epoch:  7  batch: 1500 loss: 0.00003858 train accuracy:  99.969%\n",
      "epoch:  7  batch: 1800 loss: 0.00001564 train accuracy:  99.974%\n",
      "epoch:  7  batch: 2100 loss: 0.00005947 train accuracy:  99.978%\n",
      "Finished epoch 8/10. Test accuracy = 98.63%\n",
      "epoch:  8  batch:  300 loss: 0.00006764 train accuracy:  99.969%\n",
      "epoch:  8  batch:  600 loss: 0.00162658 train accuracy:  99.880%\n",
      "epoch:  8  batch:  900 loss: 0.00011902 train accuracy:  99.878%\n",
      "epoch:  8  batch: 1200 loss: 0.00003073 train accuracy:  99.909%\n",
      "epoch:  8  batch: 1500 loss: 0.00003368 train accuracy:  99.927%\n",
      "epoch:  8  batch: 1800 loss: 0.00010152 train accuracy:  99.939%\n",
      "epoch:  8  batch: 2100 loss: 0.00004381 train accuracy:  99.948%\n",
      "Finished epoch 9/10. Test accuracy = 98.87%\n",
      "epoch:  9  batch:  300 loss: 0.00004548 train accuracy:  99.979%\n",
      "epoch:  9  batch:  600 loss: 0.00005989 train accuracy:  99.990%\n",
      "epoch:  9  batch:  900 loss: 0.00005723 train accuracy:  99.979%\n",
      "epoch:  9  batch: 1200 loss: 0.00017545 train accuracy:  99.984%\n",
      "epoch:  9  batch: 1500 loss: 0.00004211 train accuracy:  99.987%\n",
      "epoch:  9  batch: 1800 loss: 0.00002104 train accuracy:  99.983%\n",
      "epoch:  9  batch: 2100 loss: 0.00100332 train accuracy:  99.985%\n",
      "Finished epoch 10/10. Test accuracy = 98.69%\n",
      "\n",
      "Duration: 6520 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "numepochs = 10\n",
    "\n",
    "# initialize losses\n",
    "CtrainLoss = torch.zeros(numepochs)\n",
    "CtestLoss  = torch.zeros(numepochs)\n",
    "CtrainAcc  = torch.zeros(numepochs)\n",
    "CtestAcc   = torch.zeros(numepochs)\n",
    "Ctrain_correct = []\n",
    "Ctest_correct = []\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "    \n",
    "    # loop over training data batches\n",
    "    cnn.train() # switch to train mode\n",
    "    batchLoss = []\n",
    "    batchAcc  = []\n",
    "    Ctrn_corr = 0\n",
    "    \n",
    "    for b, (X,y) in enumerate(train_loader):\n",
    "        b += 1\n",
    "        # forward pass and loss\n",
    "        yHat = cnn(X)\n",
    "        loss = lossfun(yHat,y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "        batch_corr = (torch.argmax(yHat,axis=1) == y).sum()\n",
    "        Ctrn_corr += batch_corr \n",
    "        \n",
    "        # Print interim results\n",
    "        if b%300 == 0:\n",
    "            print(f'epoch: {epochi:2}  batch: {b:4} loss: {loss.item():10.8f} train accuracy: {Ctrn_corr.item()*100/(32*b):7.3f}%')\n",
    "\n",
    "      # end of batch loop...\n",
    "\n",
    "    # and get average losses and accuracies across the batches\n",
    "    CtrainLoss[epochi] = np.mean(batchLoss)\n",
    "    CtrainAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    \n",
    "    #### test performance (here done in batches!)\n",
    "    cnn.eval() # switch to test mode\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    Ctst_corr = 0\n",
    "    \n",
    "    for b,(X,y) in enumerate(test_loader):\n",
    "        # forward pass and loss\n",
    "        with torch.no_grad():\n",
    "            yHat = cnn(X)\n",
    "            loss = lossfun(yHat,y)\n",
    "            Ctst_corr += (torch.argmax(yHat,axis=1) == y).sum()\n",
    "            \n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    \n",
    "    # and get average losses and accuracies across the batches\n",
    "    CtestLoss[epochi] = np.mean(batchLoss)\n",
    "    CtestAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    # print out a status update\n",
    "    print(f'Finished epoch {epochi+1}/{numepochs}. Test accuracy = {CtestAcc[epochi]:.2f}%')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b626c58",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a756a68",
   "metadata": {},
   "source": [
    "## Basic CNN with Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704e4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.CenterCrop(100),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(hue=0.4),\n",
    "    transforms.ColorJitter(contrast=0.5),\n",
    "    transforms.ToTensor(), # normalizes to range [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # further normalization\n",
    "])\n",
    "\n",
    "# test transformations\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.CenterCrop(100),\n",
    "    transforms.ToTensor(), # normalizes to range [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # further normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eeec13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images available: 67692\n",
      "Testing images available:  22688\n",
      "Class types available:  131\n"
     ]
    }
   ],
   "source": [
    "root = '../Data/Fruit360/fruits-360_dataset/fruits-360/'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Training'), transform=transform_train)\n",
    "test_data = datasets.ImageFolder(os.path.join(root, 'Test'), transform=transform_test)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=True)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "\n",
    "print(f'Training images available: {len(train_data)}')\n",
    "print(f'Testing images available:  {len(test_data)}')\n",
    "print(f'Class types available:  {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92fd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        ### -------------- feature map layers -------------- ###\n",
    "        # first convolution layer\n",
    "        self.conv1  = nn.Conv2d(3,32,3,stride=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(32) # input the number of channels in this layer\n",
    "      \n",
    "\n",
    "        # second convolution layer\n",
    "        self.conv2  = nn.Conv2d(32,32,3,stride=1)\n",
    "        self.bnorm2 = nn.BatchNorm2d(32) # input the number of channels in this layer\n",
    "      \n",
    "        # third convolution layer\n",
    "        self.conv3  = nn.Conv2d(32,64,3,stride=1)\n",
    "        self.bnorm3 = nn.BatchNorm2d(64) # input the number of channels in this layer\n",
    "\n",
    "\n",
    "        ### -------------- linear decision layers -------------- ###\n",
    "        self.fc1 = nn.Linear(10*10*64,1024)\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512,131)\n",
    " \n",
    "\n",
    "    def forward(self,x):\n",
    " \n",
    "      \n",
    "        # first block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv1(x),2)\n",
    "        x = F.relu(self.bnorm1(x))\n",
    "\n",
    "        # second block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv2(x),2)\n",
    "        x = F.relu(self.bnorm2(x)) \n",
    "\n",
    "        # third block: convolution -> maxpool -> batchnorm -> relu\n",
    "        x = F.max_pool2d(self.conv3(x),2)\n",
    "        x = F.relu(self.bnorm3(x))\n",
    "  \n",
    "        # reshape for linear layer\n",
    "        x = x.view(-1,10*10*64)\n",
    "      \n",
    "        # linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# create the model instance\n",
    "cnn = basicCNN()\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(),lr=0.01,momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c5a2e27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  300 loss: 0.71017039 train accuracy:  42.023%\n",
      "epoch:  0  batch:  600 loss: 0.20436464 train accuracy:  64.802%\n",
      "Finished epoch 1/10. Test accuracy = 85.65%\n",
      "epoch:  1  batch:  300 loss: 0.07304914 train accuracy:  96.473%\n",
      "epoch:  1  batch:  600 loss: 0.03413671 train accuracy:  97.183%\n",
      "Finished epoch 2/10. Test accuracy = 85.69%\n",
      "epoch:  2  batch:  300 loss: 0.01299843 train accuracy:  98.570%\n",
      "epoch:  2  batch:  600 loss: 0.00482858 train accuracy:  98.893%\n",
      "Finished epoch 3/10. Test accuracy = 94.57%\n",
      "epoch:  3  batch:  300 loss: 0.02039841 train accuracy:  99.250%\n",
      "epoch:  3  batch:  600 loss: 0.04747273 train accuracy:  99.277%\n",
      "Finished epoch 4/10. Test accuracy = 92.64%\n",
      "epoch:  4  batch:  300 loss: 0.01539907 train accuracy:  99.497%\n",
      "epoch:  4  batch:  600 loss: 0.00806915 train accuracy:  99.582%\n",
      "Finished epoch 5/10. Test accuracy = 94.56%\n",
      "epoch:  5  batch:  300 loss: 0.00713794 train accuracy:  99.777%\n",
      "epoch:  5  batch:  600 loss: 0.00366056 train accuracy:  99.740%\n",
      "Finished epoch 6/10. Test accuracy = 96.50%\n",
      "epoch:  6  batch:  300 loss: 0.00875930 train accuracy:  99.827%\n",
      "epoch:  6  batch:  600 loss: 0.01241804 train accuracy:  99.818%\n",
      "Finished epoch 7/10. Test accuracy = 96.62%\n",
      "epoch:  7  batch:  300 loss: 0.00157099 train accuracy:  99.907%\n",
      "epoch:  7  batch:  600 loss: 0.00058926 train accuracy:  99.930%\n",
      "Finished epoch 8/10. Test accuracy = 95.83%\n",
      "epoch:  8  batch:  300 loss: 0.00073804 train accuracy:  99.947%\n",
      "epoch:  8  batch:  600 loss: 0.00162147 train accuracy:  99.953%\n",
      "Finished epoch 9/10. Test accuracy = 96.57%\n",
      "epoch:  9  batch:  300 loss: 0.00516279 train accuracy:  99.897%\n",
      "epoch:  9  batch:  600 loss: 0.00065193 train accuracy:  99.877%\n",
      "Finished epoch 10/10. Test accuracy = 96.14%\n",
      "\n",
      "Duration: 6942 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "numepochs = 10\n",
    "\n",
    "# initialize losses\n",
    "CtrainLoss = torch.zeros(numepochs)\n",
    "CtestLoss  = torch.zeros(numepochs)\n",
    "CtrainAcc  = torch.zeros(numepochs)\n",
    "CtestAcc   = torch.zeros(numepochs)\n",
    "Ctrain_correct = []\n",
    "Ctest_correct = []\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "    \n",
    "    # loop over training data batches\n",
    "    cnn.train() # switch to train mode\n",
    "    batchLoss = []\n",
    "    batchAcc  = []\n",
    "    Ctrn_corr = 0\n",
    "    \n",
    "    for b, (X,y) in enumerate(train_loader):\n",
    "        b += 1\n",
    "        # forward pass and loss\n",
    "        yHat = cnn(X)\n",
    "        loss = lossfun(yHat,y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "        batch_corr = (torch.argmax(yHat,axis=1) == y).sum()\n",
    "        Ctrn_corr += batch_corr \n",
    "        \n",
    "        # Print interim results\n",
    "        if b%300 == 0:\n",
    "            print(f'epoch: {epochi:2}  batch: {b:4} loss: {loss.item():10.8f} train accuracy: {Ctrn_corr.item()*100/(100*b):7.3f}%')\n",
    "\n",
    "      # end of batch loop...\n",
    "\n",
    "    # and get average losses and accuracies across the batches\n",
    "    CtrainLoss[epochi] = np.mean(batchLoss)\n",
    "    CtrainAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    \n",
    "    #### test performance (here done in batches!)\n",
    "    cnn.eval() # switch to test mode\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    Ctst_corr = 0\n",
    "    \n",
    "    for b,(X,y) in enumerate(test_loader):\n",
    "        # forward pass and loss\n",
    "        with torch.no_grad():\n",
    "            yHat = cnn(X)\n",
    "            loss = lossfun(yHat,y)\n",
    "            Ctst_corr += (torch.argmax(yHat,axis=1) == y).sum()\n",
    "            \n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    \n",
    "    # and get average losses and accuracies across the batches\n",
    "    CtestLoss[epochi] = np.mean(batchLoss)\n",
    "    CtestAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    # print out a status update\n",
    "    print(f'Finished epoch {epochi+1}/{numepochs}. Test accuracy = {CtestAcc[epochi]:.2f}%')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd9185",
   "metadata": {},
   "source": [
    "## VGG16 Improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fedef025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=131, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel2 = vgg\n",
    "vggmodel2.load_state_dict(torch.load('FRUITS-VGG-Model.pt'))\n",
    "vggmodel2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "abc0bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(vggmodel2.parameters(),lr=0.001,momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "71704fb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  100 loss: 0.03889561 train accuracy:  92.770%\n",
      "epoch:  0  batch:  200 loss: 0.07846430 train accuracy:  95.405%\n",
      "epoch:  0  batch:  300 loss: 0.01480018 train accuracy:  96.457%\n",
      "epoch:  0  batch:  400 loss: 0.01963034 train accuracy:  97.052%\n",
      "epoch:  0  batch:  500 loss: 0.00926822 train accuracy:  97.536%\n",
      "epoch:  0  batch:  600 loss: 0.00087650 train accuracy:  97.892%\n",
      "Finished epoch 1/6. Test accuracy = 98.13%\n",
      "epoch:  1  batch:  100 loss: 0.00063328 train accuracy:  99.760%\n",
      "epoch:  1  batch:  200 loss: 0.00115976 train accuracy:  99.745%\n",
      "epoch:  1  batch:  300 loss: 0.00023939 train accuracy:  99.750%\n",
      "epoch:  1  batch:  400 loss: 0.03106214 train accuracy:  99.725%\n",
      "epoch:  1  batch:  500 loss: 0.06194613 train accuracy:  99.732%\n",
      "epoch:  1  batch:  600 loss: 0.00112885 train accuracy:  99.752%\n",
      "Finished epoch 2/6. Test accuracy = 98.62%\n",
      "epoch:  2  batch:  100 loss: 0.02393621 train accuracy:  99.680%\n",
      "epoch:  2  batch:  200 loss: 0.00058390 train accuracy:  99.805%\n",
      "epoch:  2  batch:  300 loss: 0.00091091 train accuracy:  99.857%\n",
      "epoch:  2  batch:  400 loss: 0.00018996 train accuracy:  99.858%\n",
      "epoch:  2  batch:  500 loss: 0.00010526 train accuracy:  99.862%\n",
      "epoch:  2  batch:  600 loss: 0.00015886 train accuracy:  99.857%\n",
      "Finished epoch 3/6. Test accuracy = 98.22%\n",
      "epoch:  3  batch:  100 loss: 0.01705959 train accuracy:  99.830%\n",
      "epoch:  3  batch:  200 loss: 0.02474511 train accuracy:  99.825%\n",
      "epoch:  3  batch:  300 loss: 0.00165131 train accuracy:  99.860%\n",
      "epoch:  3  batch:  400 loss: 0.00020756 train accuracy:  99.877%\n",
      "epoch:  3  batch:  500 loss: 0.03793039 train accuracy:  99.888%\n",
      "epoch:  3  batch:  600 loss: 0.00001233 train accuracy:  99.900%\n",
      "Finished epoch 4/6. Test accuracy = 98.70%\n",
      "epoch:  4  batch:  100 loss: 0.00027893 train accuracy:  99.920%\n",
      "epoch:  4  batch:  200 loss: 0.00201716 train accuracy:  99.850%\n",
      "epoch:  4  batch:  300 loss: 0.00893511 train accuracy:  99.853%\n",
      "epoch:  4  batch:  400 loss: 0.00008247 train accuracy:  99.865%\n",
      "epoch:  4  batch:  500 loss: 0.00007222 train accuracy:  99.854%\n",
      "epoch:  4  batch:  600 loss: 0.00027516 train accuracy:  99.858%\n",
      "Finished epoch 5/6. Test accuracy = 98.80%\n",
      "epoch:  5  batch:  100 loss: 0.00012257 train accuracy:  99.990%\n",
      "epoch:  5  batch:  200 loss: 0.00292560 train accuracy:  99.975%\n",
      "epoch:  5  batch:  300 loss: 0.00006905 train accuracy:  99.980%\n",
      "epoch:  5  batch:  400 loss: 0.00027688 train accuracy:  99.955%\n",
      "epoch:  5  batch:  500 loss: 0.00000182 train accuracy:  99.962%\n",
      "epoch:  5  batch:  600 loss: 0.00006785 train accuracy:  99.958%\n",
      "Finished epoch 6/6. Test accuracy = 98.98%\n",
      "\n",
      "Duration: 60091 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "numepochs = 6\n",
    "\n",
    "# initialize losses\n",
    "VNtrainLoss = torch.zeros(numepochs)\n",
    "VNtestLoss  = torch.zeros(numepochs)\n",
    "VNtrainAcc  = torch.zeros(numepochs)\n",
    "VNtestAcc   = torch.zeros(numepochs)\n",
    "VNtrain_correct = []\n",
    "VNtest_correct = []\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    \n",
    "    # loop over training data batches\n",
    "    vggmodel2.train() # switch to train mode\n",
    "    batchLoss = []\n",
    "    batchAcc  = []\n",
    "    VNtrn_corr = 0\n",
    "    \n",
    "    for b, (X,y) in enumerate(train_loader):\n",
    "        \n",
    "        b += 1\n",
    "        # forward pass and loss\n",
    "        yHat = vggmodel2(X)\n",
    "        loss = lossfun(yHat,y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "        batch_corr = (torch.argmax(yHat,axis=1) == y).sum()\n",
    "        VNtrn_corr += batch_corr \n",
    "        \n",
    "        # Print interim results\n",
    "        if b%100 == 0:\n",
    "            print(f'epoch: {epochi:2}  batch: {b:4} loss: {loss.item():10.8f} train accuracy: {VNtrn_corr.item()*100/(100*b):7.3f}%')\n",
    "\n",
    "      # end of batch loop...\n",
    "\n",
    "    # and get average losses and accuracies across the batches\n",
    "    VNtrainLoss[epochi] = np.mean(batchLoss)\n",
    "    VNtrainAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    \n",
    "    #### test performance (here done in batches!)\n",
    "    vggmodel2.eval() # switch to test mode\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    VNtst_corr = 0\n",
    "    \n",
    "    for b,(X,y) in enumerate(test_loader):\n",
    "        # forward pass and loss\n",
    "        with torch.no_grad():\n",
    "            yHat = vggmodel2(X)\n",
    "            loss = lossfun(yHat,y)\n",
    "            VNtst_corr += (torch.argmax(yHat,axis=1) == y).sum()\n",
    "            \n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    \n",
    "    # and get average losses and accuracies across the batches\n",
    "    VNtestLoss[epochi] = np.mean(batchLoss)\n",
    "    VNtestAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    # print out a status update\n",
    "    print(f'Finished epoch {epochi+1}/{numepochs}. Test accuracy = {VNtestAcc[epochi]:.2f}%')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4a6da",
   "metadata": {},
   "source": [
    "## ResNet 34 Improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8568e991",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=131, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34model2 = resnet\n",
    "resnet34model2.load_state_dict(torch.load('FRUITS-RESNET34-Model.pt'))\n",
    "resnet34model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6954b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet34model2.parameters(),lr=0.001,momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54419e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  100 loss: 0.01247818 train accuracy:  99.990%\n",
      "epoch:  0  batch:  200 loss: 0.00635256 train accuracy:  99.990%\n",
      "epoch:  0  batch:  300 loss: 0.00821499 train accuracy:  99.983%\n",
      "epoch:  0  batch:  400 loss: 0.00704362 train accuracy:  99.985%\n",
      "epoch:  0  batch:  500 loss: 0.00869639 train accuracy:  99.986%\n",
      "epoch:  0  batch:  600 loss: 0.01135406 train accuracy:  99.987%\n",
      "Finished epoch 1/6. Test accuracy = 97.62%\n",
      "epoch:  1  batch:  100 loss: 0.00493810 train accuracy:  99.990%\n",
      "epoch:  1  batch:  200 loss: 0.00983767 train accuracy:  99.990%\n",
      "epoch:  1  batch:  300 loss: 0.00622975 train accuracy:  99.987%\n",
      "epoch:  1  batch:  400 loss: 0.00837342 train accuracy:  99.990%\n",
      "epoch:  1  batch:  500 loss: 0.00714298 train accuracy:  99.988%\n",
      "epoch:  1  batch:  600 loss: 0.01094252 train accuracy:  99.990%\n",
      "Finished epoch 2/6. Test accuracy = 97.48%\n",
      "epoch:  2  batch:  100 loss: 0.00972671 train accuracy: 100.000%\n",
      "epoch:  2  batch:  200 loss: 0.00595572 train accuracy:  99.995%\n",
      "epoch:  2  batch:  300 loss: 0.00381534 train accuracy:  99.993%\n",
      "epoch:  2  batch:  400 loss: 0.00652572 train accuracy:  99.995%\n",
      "epoch:  2  batch:  500 loss: 0.00521796 train accuracy:  99.992%\n",
      "epoch:  2  batch:  600 loss: 0.00431107 train accuracy:  99.993%\n",
      "Finished epoch 3/6. Test accuracy = 97.58%\n",
      "epoch:  3  batch:  100 loss: 0.00781758 train accuracy: 100.000%\n",
      "epoch:  3  batch:  200 loss: 0.00791883 train accuracy:  99.995%\n",
      "epoch:  3  batch:  300 loss: 0.00640439 train accuracy:  99.993%\n",
      "epoch:  3  batch:  400 loss: 0.00455099 train accuracy:  99.990%\n",
      "epoch:  3  batch:  500 loss: 0.00858816 train accuracy:  99.992%\n",
      "epoch:  3  batch:  600 loss: 0.00714209 train accuracy:  99.990%\n",
      "Finished epoch 4/6. Test accuracy = 97.44%\n",
      "epoch:  4  batch:  100 loss: 0.00781971 train accuracy: 100.000%\n",
      "epoch:  4  batch:  200 loss: 0.00834652 train accuracy: 100.000%\n",
      "epoch:  4  batch:  300 loss: 0.00704765 train accuracy:  99.997%\n",
      "epoch:  4  batch:  400 loss: 0.00804818 train accuracy:  99.995%\n",
      "epoch:  4  batch:  500 loss: 0.00406460 train accuracy:  99.994%\n",
      "epoch:  4  batch:  600 loss: 0.01344630 train accuracy:  99.995%\n",
      "Finished epoch 5/6. Test accuracy = 97.51%\n",
      "epoch:  5  batch:  100 loss: 0.01040020 train accuracy:  99.980%\n",
      "epoch:  5  batch:  200 loss: 0.00867816 train accuracy:  99.985%\n",
      "epoch:  5  batch:  300 loss: 0.00924506 train accuracy:  99.990%\n",
      "epoch:  5  batch:  400 loss: 0.00586261 train accuracy:  99.990%\n",
      "epoch:  5  batch:  500 loss: 0.00581309 train accuracy:  99.992%\n",
      "epoch:  5  batch:  600 loss: 0.00577007 train accuracy:  99.993%\n",
      "Finished epoch 6/6. Test accuracy = 97.46%\n",
      "\n",
      "Duration: 13352 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "numepochs = 6\n",
    "\n",
    "# initialize losses\n",
    "RNtrainLoss = torch.zeros(numepochs)\n",
    "RNtestLoss  = torch.zeros(numepochs)\n",
    "RNtrainAcc  = torch.zeros(numepochs)\n",
    "RNtestAcc   = torch.zeros(numepochs)\n",
    "RNtrain_correct = []\n",
    "RNtest_correct = []\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    \n",
    "    # loop over training data batches\n",
    "    resnet34model2.train() # switch to train mode\n",
    "    batchLoss = []\n",
    "    batchAcc  = []\n",
    "    RNtrn_corr = 0\n",
    "    \n",
    "    for b, (X,y) in enumerate(train_loader):\n",
    "        \n",
    "        b += 1\n",
    "        # forward pass and loss\n",
    "        yHat = resnet34model2(X)\n",
    "        loss = lossfun(yHat,y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "        batch_corr = (torch.argmax(yHat,axis=1) == y).sum()\n",
    "        RNtrn_corr += batch_corr \n",
    "        \n",
    "        # Print interim results\n",
    "        if b%100 == 0:\n",
    "            print(f'epoch: {epochi:2}  batch: {b:4} loss: {loss.item():10.8f} train accuracy: {RNtrn_corr.item()*100/(100*b):7.3f}%')\n",
    "\n",
    "      # end of batch loop...\n",
    "\n",
    "    # and get average losses and accuracies across the batches\n",
    "    RNtrainLoss[epochi] = np.mean(batchLoss)\n",
    "    RNtrainAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    \n",
    "    #### test performance (here done in batches!)\n",
    "    resnet34model2.eval() # switch to test mode\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    RNtst_corr = 0\n",
    "    \n",
    "    for b,(X,y) in enumerate(test_loader):\n",
    "        # forward pass and loss\n",
    "        with torch.no_grad():\n",
    "            yHat = resnet34model2(X)\n",
    "            loss = lossfun(yHat,y)\n",
    "            RNtst_corr += (torch.argmax(yHat,axis=1) == y).sum()\n",
    "            \n",
    "        # loss and accuracy from this batch\n",
    "        batchLoss.append(loss.item())\n",
    "        batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    \n",
    "    # and get average losses and accuracies across the batches\n",
    "    RNtestLoss[epochi] = np.mean(batchLoss)\n",
    "    RNtestAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "    # print out a status update\n",
    "    print(f'Finished epoch {epochi+1}/{numepochs}. Test accuracy = {RNtestAcc[epochi]:.2f}%')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
